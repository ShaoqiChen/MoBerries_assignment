{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3810jvsc74a57bd07f6ceab5b112bc4d419fb57b0acece2d8d8fa91bd9372d3d0d53d7bb6f6a4e32",
   "display_name": "Python 3.8.10 64-bit ('tweet_sentiment': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re"
   ]
  },
  {
   "source": [
    "### First of all, load dataset and select the relevant columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Sentiment Analysis Dataset.csv')\n",
    "# Keeping only the neccessary columns\n",
    "data = data[['SentimentText','Sentiment']]"
   ]
  },
  {
   "source": [
    "### Keep the Sentiment column as label, and process the text in a way that only letters, !? symbols and space remain. The text is then processed by Toenizer to be converted into a sequence which is fed into neural network as input"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "790185\n1576880\n"
     ]
    }
   ],
   "source": [
    "data['SentimentText'] = data['SentimentText'].apply(lambda x: x.lower())\n",
    "data['SentimentText'] = data['SentimentText'].apply((lambda x: re.sub('[^a-zA-z!?\\s]','',x)))\n",
    "\n",
    "print(len(data[ data['Sentiment'] == 1]))\n",
    "print(data[ data['Sentiment'] == 0].size)\n",
    "\n",
    "for idx,row in data.iterrows():\n",
    "    row[0] = row[0].replace('rt',' ')\n",
    "    \n",
    "max_fatures = 20000\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(data['SentimentText'].values)\n",
    "X = tokenizer.texts_to_sequences(data['SentimentText'].values)\n",
    "X = pad_sequences(X)"
   ]
  },
  {
   "source": [
    "### Next, build a neural network based on LSTM. Note tha dropout, lstm_out, batch_size, embed_dim and optimizer are hayperparameters, they should be tuned well to get the best performance. The loss function is softmax which predicts the probability for each class and is specially for classification."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 43, 128)           2560000   \n_________________________________________________________________\nlstm (LSTM)                  (None, 196)               254800    \n_________________________________________________________________\ndense (Dense)                (None, 2)                 394       \n=================================================================\nTotal params: 2,815,194\nTrainable params: 2,815,194\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1],))\n",
    "model.add(LSTM(lstm_out, dropout=0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "source": [
    "### The training dataset and test dataset:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1262900, 43) (1262900, 2)\n(315725, 43) (315725, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Y = pd.get_dummies(data['Sentiment']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "source": [
    "### Perform training and save the trained model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/7\n",
      "9867/9867 - 2533s - loss: 0.4189 - accuracy: 0.8068\n",
      "Epoch 2/7\n",
      "9867/9867 - 2036s - loss: 0.3765 - accuracy: 0.8304\n",
      "Epoch 3/7\n",
      "9867/9867 - 2122s - loss: 0.3524 - accuracy: 0.8434\n",
      "Epoch 4/7\n",
      "9867/9867 - 2233s - loss: 0.3308 - accuracy: 0.8546\n",
      "Epoch 5/7\n",
      "9867/9867 - 2004s - loss: 0.3106 - accuracy: 0.8647\n",
      "Epoch 6/7\n",
      "9867/9867 - 2017s - loss: 0.2918 - accuracy: 0.8742\n",
      "Epoch 7/7\n",
      "9867/9867 - 4444s - loss: 0.2758 - accuracy: 0.8818\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "INFO:tensorflow:Assets written to: sentiment_lstm/assets\n",
      "INFO:tensorflow:Assets written to: sentiment_lstm/assets\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "model.fit(X_train, Y_train, epochs = 7, batch_size=batch_size, verbose = 2)\n",
    "model.save('sentiment_lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1234/1234 - 73s - loss: 0.4248 - accuracy: 0.8211\n",
      "score: 0.42\n",
      "acc: 0.82\n"
     ]
    }
   ],
   "source": [
    "\n",
    "validation_size = int(len(X_test)/2)\n",
    "\n",
    "X_validate = X_test[-validation_size:]\n",
    "Y_validate = Y_test[-validation_size:]\n",
    "X_test = X_test[:-validation_size]\n",
    "Y_test = Y_test[:-validation_size]\n",
    "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))"
   ]
  },
  {
   "source": [
    "## Observation:\n",
    "Accuracy on test set is smaller than that on training dataset which indicates the model is overfitting in the training process. Nevertheless, in comparision to Navie Bayes (accuracy:65%) and SVM (accuracy:68%) in scikitlearn, there is doubtlessly improvement in the performace of lstm model (accuracy:82%) for this sentiment analysis task"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### If we dive into the performance of classification, it can be observed that for different type of sentence, i.e. positive and negative, the accuracy is slightly diferent: accuracy for positive sentence is larger than accuracy for negative. By looking at negative words, like 'terrible', 'destroy', model can get wrong prrdiction easily even though the sentence is positive."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4934/4934 - 110s\n"
     ]
    }
   ],
   "source": [
    "result = model.predict(X_validate,verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = np.argmax(result, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_label = np.argmax(Y_validate,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Pos: 79037\npos_cnt: 65044\npos_acc: 0.8229563369055\n"
     ]
    }
   ],
   "source": [
    "pos_true_index = true_label == 1\n",
    "print('Pos:',np.sum(pos_true_index))\n",
    "pos_cnt = np.sum(predict[pos_true_index]==true_label[pos_true_index])\n",
    "print('pos_cnt:',pos_cnt)\n",
    "print('pos_acc:',pos_cnt/np.sum(pos_true_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Neg: 78825\nNeg_cnt: 64456\nNeg_acc: 0.8177101173485569\n"
     ]
    }
   ],
   "source": [
    "pos_true_index = true_label == 0\n",
    "print('Neg:',np.sum(pos_true_index))\n",
    "pos_cnt = np.sum(predict[pos_true_index]==true_label[pos_true_index])\n",
    "print('Neg_cnt:',pos_cnt)\n",
    "print('Neg_acc:',pos_cnt/np.sum(pos_true_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "supposed to be seeing at tonight but might have to cancel if the weather stays this lame\nprediction: 0\nTrue: 0\ncould be data can usually be taken both ways in these kinds of things lets say its generally\nprediction: 1\nTrue: 1\nhehe yeah days ago i put it to a stop but i still got used to via to make things flow\nprediction: 1\nTrue: 1\nto the gym i feel like its gonna be a good workout today\nprediction: 1\nTrue: 1\nthe canucks looked terrible tonight save for the st period however seeing all those white towels live is still pretty amazing\nprediction: 0\nTrue: 1\now itchy hay fever eyes for the lose\nprediction: 0\nTrue: 0\ni have homework to do why was school invented haaa xx\nprediction: 1\nTrue: 0\nis up feeding kaylee\nprediction: 1\nTrue: 1\nhi guys cant wait to see the show i just know its going to be fantastic\nprediction: 1\nTrue: 1\nthere are who tweet perhaps you could do a\nprediction: 1\nTrue: 1\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(tokenizer.sequences_to_texts(X_validate[:10])):\n",
    "    print(sentence)\n",
    "    print('prediction:',predict[idx])\n",
    "    print('True:',true_label[idx])"
   ]
  },
  {
   "source": [
    "### As an extra step, let's see how the training performs visually in Tensorboard"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension.\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TensorFlow version:  2.5.0\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from packaging import version\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "    \"This notebook requires TensorFlow 2.0 or above.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "max_fatures= 20000\n",
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "batch_size= 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    Embedding(max_fatures, embed_dim,input_length = X.shape[1],),\n",
    "    LSTM(lstm_out, dropout=0.2),\n",
    "    Dense(2,activation='softmax'),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss = 'categorical_crossentropy', \n",
    "    optimizer='adam',\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Training ... With default parameters, this takes less than 10 seconds.\")\n",
    "training_history = model.fit(\n",
    "    X_train, # input\n",
    "    Y_train, # output\n",
    "    batch_size=batch_size,\n",
    "    verbose=0, # Suppress chatty output; use Tensorboard instead\n",
    "    epochs=5,\n",
    "    validation_data=(X_validate, Y_validate),\n",
    "    callbacks=[tensorboard_callback],\n",
    ")\n",
    "\n",
    "print(\"Average test loss: \", np.average(training_history.history['loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "ERROR: Could not find `tensorboard`. Please ensure that your PATH\ncontains an executable `tensorboard` program, or explicitly specify\nthe path to a TensorBoard binary by setting the `TENSORBOARD_BINARY`\nenvironment variable."
     },
     "metadata": {}
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "source": [
    "## Comments\n",
    "\n",
    "- Must save trained model into a path, in case it is needed to load the model afterwards\n",
    "- In the very few first steps, the model gets overfitted, we have to introduce a regularization term to av"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Reference\n",
    "- [Sentment Analysis with LSTM by Peter Nagy ](https://github.com/nagypeterjob/Sentiment-Analysis-NLTK-ML-LSTM/blob/master/lstm.ipynb)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}