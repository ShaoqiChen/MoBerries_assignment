{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python377jvsc74a57bd0ff34381bf7cd392e110dc97b26dfa36a9d25e5d0f0570a600d6d90e5edf8edf7",
   "display_name": "Python 3.7.7 64-bit ('tf-keras': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "source": [
    "# 1. Load dataset and preprocess text"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Sentiment Analysis Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "#### Removing punctuation\n",
    "# function to remove special characters\n",
    "def remove_special_characters(text):\n",
    "    # define the pattern to keep\n",
    "    pat = r'[^a-zA-z!?\\'\\s]' \n",
    "    return re.sub(pat, '', text)\n",
    "\n",
    "\n",
    "#### Lemmatization\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# function to lemmatize words\n",
    "def get_lem(text):\n",
    "    text = nlp(text)\n",
    "\n",
    "\n",
    "    text = [word.lemma_.lower() if word.lemma_ != '-PRON-' else word.text for word in text]\n",
    "    text_without_space = [word for word in text if not word.isspace()]\n",
    "\n",
    "    return ' '.join(text_without_space)\n",
    "\n",
    "# combine above two functions\n",
    "def cleanup(text):\n",
    "    text = remove_special_characters(text)\n",
    "    text = get_lem(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "source": [
    "### If there is no 'Text.csv', normalize text in dataset and save into 'Text.csv'"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(int(len(df)/5000)):\n",
    "    if (i+1)*5000 < len(df):\n",
    "        df['Text_normalized'][i*5000:(i+1)*5000] = df['SentimentText'][i*5000:(i+1)*5000].apply(cleanup).values\n",
    "        print('Finish {} texts'.format((i+1)*5000))\n",
    "    else: \n",
    "        df['Text_normalized'][i*5000:] = df['SentimentText'][i*5000:].apply(cleanup).values\n",
    "        print('It is done!')\n",
    "df.to_csv('Text.csv')"
   ]
  },
  {
   "source": [
    "### If 'Text.csv' exists, load csv:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = pd.read_csv('Text.csv')\n",
    "\n",
    "text_data = df_text.loc[:10000,'Text_normalized'].values # features for training and testing\n",
    "text_label = df.loc[:10000,'Sentiment'].values # labels for training and testing\n",
    "# on my computer when large amount of dataset is loaded, the computer crashed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(9000,)\n(9000,)\n(1001,)\n(1001,)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'a baby fall flat on his face and start bawl because of i forget that he be wobbley !'"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split features and labels into two datasets, namely training dataset and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data, text_label, test_size=0.1, random_state=1)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "X_train[0]"
   ]
  },
  {
   "source": [
    "# 2. Convert text into feature vectors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(9000, 16007)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# apply 'bag of words' method to convert a sentence into a feature vector\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train) # learning the vocabulary dictionary and return a matrix\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(9000, 16007)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# apply term frequency(TF) times inverse document frequency(IDF) to avoid imbalanced weights od words in longer sentence from shorted sentence.\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "source": [
    "# 3. Build models\n",
    "\n",
    "### We use two simple classification models implemented in scikit-learn\n",
    "- Navie Bayes\n",
    "- Support Vector Machine"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3.1 Naive Bayes 'GaussianNB()'"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('standardscaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('gaussiannb', GaussianNB(priors=None, var_smoothing=1e-09))],\n",
       "         verbose=False)"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "#training\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "clf_nb = make_pipeline(StandardScaler(), GaussianNB())\n",
    "\n",
    "clf_nb.fit(X_train_tfidf.toarray(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6493506493506493"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# Prediction of GaussianNB\n",
    "X_test_counts = count_vect.transform(X_test)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "predicted_nb = clf_nb.predict(X_test_tfidf.toarray())\n",
    "np.mean(predicted_nb == y_test)"
   ]
  },
  {
   "source": [
    "### Naive Bayes gives 64.9 % accuracy when tested on 1001 samples"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3.2 Support Vector Machine"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6823176823176823"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "# we are using linear SVM\n",
    "clf_sgd = make_pipeline(StandardScaler(), SGDClassifier(loss='hinge', max_iter=500, tol=1e-3))\n",
    "clf_sgd.fit(X_train_tfidf.toarray(), y_train)\n",
    "predicted_sgd = clf_sgd.predict(X_test_tfidf.toarray())\n",
    "np.mean(predicted_sgd == y_test)"
   ]
  },
  {
   "source": [
    "### In contrast to NB, SGD classifer obtains a higher accurancy of 68.2 % on 1001 samples\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 4. Evaluation of performance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 4.1 Naive Bayes Performance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n    Positive       0.65      0.86      0.74       585\n    Negative       0.64      0.35      0.45       416\n\n    accuracy                           0.65      1001\n   macro avg       0.65      0.61      0.60      1001\nweighted avg       0.65      0.65      0.62      1001\n\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, predicted_nb, target_names=['Positive','Negative']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[505,  80],\n",
       "       [271, 145]])"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, predicted_nb)"
   ]
  },
  {
   "source": [
    "## 4.2 SVM Performance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n    Positive       0.66      0.93      0.77       585\n    Negative       0.78      0.33      0.46       416\n\n    accuracy                           0.68      1001\n   macro avg       0.72      0.63      0.62      1001\nweighted avg       0.71      0.68      0.65      1001\n\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, predicted_sgd, target_names=['Positive','Negative']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[545,  40],\n",
       "       [278, 138]])"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, predicted_sgd)"
   ]
  },
  {
   "source": [
    "### From the perspective of f1 score, f1 score of SGDClassifier is bigger than the other and closer to 1, which also indicates the latter has a better performance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 5 Parameter tuning\n",
    "\n",
    "### When it comes to model performance, it also depends on the parameter used to build the models, like penality in SGDClassifier. The penality have an impact on the training process and can lead to better generalization. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# we use grid search to select best parameters\n",
    "parameters = {\n",
    "    'alpha': (1e-2, 1e-3),\n",
    "    'loss': ('hinge', 'log')\n",
    "}\n",
    "gs_clf = GridSearchCV(clf_sgd, parameters, cv=5, n_jobs=-1) \n",
    "gs_clf = gs_clf.fit(X_train_tfidf[:5000], y_train[:400]) # return best score and best paramters"
   ]
  },
  {
   "source": [
    "# 6 Next steps"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## - the very first step shoud be making use of full dataset. \n",
    "\n",
    "I assume that memory on my computer is very limited, as the sign 'Your disk is almost full' keeps showing up. The dataset can not be processed in memory\n",
    "\n",
    "## - Emsemble models. \n",
    "\n",
    "As the performance of the simple classification model is not good and accurancy is low, we definitely take a little complext models into account. When it comes to complexity, one way to improve the performance is to train many simple models and put them together. In this way, the variance of each model is likely to be reduced.\n",
    "\n",
    "## - Neural networks. \n",
    "\n",
    "There is no doubt that we should use neural network to improve the performance, Like LSTM. In this case, logistic function can be used as loss function and Adam with L2 can be used as optimization method. The performance must be promising.\n",
    "\n",
    "## - Pretrained models. \n",
    "\n",
    "Obvioudly, there are models trained specifically for NLP tasks. In spacy or NLTK, people can load these pretrained models to do classification and finetune the parameters based on dataset we have.\n",
    "\n",
    "## - Production. \n",
    "\n",
    "Once we have a model which is trained well, we might consider to give others the access to the model. In this case, as far as I know, building a api, like by using FastAPI, will be a good starting point.\n",
    "\n",
    "## - Monitoring.\n",
    "\n",
    "Building a model can not be a final point. We should make use of the performance and feedback from people who are using the model to improve the model. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}